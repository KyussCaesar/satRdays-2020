high level:

- what _is_ production?
  what's stopping you from just running it on your machine?
  why does anything need to change?
- ans: so you can call something "finished"
  don't need to babysit it, can get on improving things
  less reliance on a single machine with your office's internet
- what makes this difficult for R compared with other languages?
  other languages:
    - e.g C: already standardised tools for building, people have
      been doing it forever
    - e.g JS: `npm` handles a lot of the complexity for you
    - e.g Rust/Go: statically-linked binary, lockfiles
  R:
    - doesn't really have any standardised approach
    - "package managers" and "version control" still seem to be
      relatively new things to the R community

---

Good morning everyone!

My name is Antony and I'm a Data Engineer at Quantiful.

<<<<<<< HEAD
Now, my primary responsibility at Quantiful is "productionising"
data science output, which, for us, means getting R code running in
production.

Yes, really, we run R code in production.

Now a lot of people generally have this reaction upon hearing that:

<why, why would you do that!?>

Well for us, we want to get better models in front of clients _as fast
as possible_, so it's no good if you develop a better model, only to
have it do _nothing_ for some time while you're busy "productionising"
it.

All of our data science work is done in R, and the fastest route to
production is just to modify that existing code rather than re-write
everything.

So if our code is already in R, then obviously the fastest route to
production is to use that code...

<right?>

<well yes, but actually no>

There's a lot more than just the code to consider; there's also the
operating system, system packages, R version, and R packages used

<too many limes>

so how can we control all of these variables? <slide>

well for code, there's <git>, pretty standard these days,

and for R packages, well the answer there's gonna be some kind of 
package manager, and for us we use <packrat>, but there are others
like R env.

and to control the other three, we use a technology called <Docker>

A quick note for those who might not know what Docker is:

<slide>

Basically, Docker is virtualisation technology, similar to a
Virtual Machine. which allows you to define isolated environments
called "containers" to run software in.

This environment is defined by a file called a Dockerfile, which
is checked-in to version control

<slide>

Docker uses this file to build a "container image", which is like
a VM image, so all of your code, dependencies, R version, stuff like
that, is frozen inside this image

we can use Docker to start "containers" from these images, which is
basically a running instance of our software

now; normally stuff with Docker only works from the command line, so
it might worry some people that they won't be able to use an IDE to
develop, but

<conviently...>

there are already container images with RStudio server installed
inside!
So you can just pull a pre-built image from the internet, with RStudio
Server already installed and set up, and run it on your machine

from there, you basically can just develop like normal

the other thing that's convenient is that container images can
sort of be "composed together", <what I have here> is an example
Dockerfile, where we start with that RStudio image and add some stuff
that we need.

in this case we're setting  up a Debian-based environment with RStudio,
R version 3.6.1, and we tell Docker "please install some packages".

<Also conveniently>, you can actually mount directories inside the container
when it starts, which means you don't have to re-build as often

so we split the build into two stages, where the first stage just acts as a 
"base" for development, and then a "standalone" stage where we copy everything
inside, and at that point <we basically have frozen everything>

...So what does all this mean?

Well, we've got the code and dependencies all very tightly controlled, in a
reproducible environment, which means that as soon as you start development

<you are already in production>

Thank you so much!! 

-----

where we landed was Git for code, Docker for the operating system,
system packages, and R version, and a library called packrat for
controlling R package versions



=======
At Quantiful, all of our analytics are done in R, and today I'm going to talk
a bit about how we get that work off of developer machines and on to production ones.

Getting code to run on a new machine is a pretty classic problem in software
engineering; basically the solution people gravitate towards these days

Basically, the answer is

Since this is a lightning talk I'll skip straight to the solution; Docker containers,

---

Good morning everyone!

My name is Antony (leave the credentials for the end slide) and today I'm going
talk a bit about how we run R code in production.

Whenver you try to run code on a new system, you're always going to run in to issues
One way to reduce the number of issues is to replicate as closely as possible the
environment in which the code was originally developed.

Essentially this breaks down into 4 variables you need to control:

- the operating system
- the R version
- the R package versions
- (of course), the code!

At my work we use Docker containers to control the first two, `packrat` for
the third, and `git` for the last.

# CRASH COURSE ON DOCKER/PACKRAT/GIT

Each of these is really a full talk in and of itself, so I'm only going to touch
on them here.

## What is Docker? What are Docker containers?

Docker containers are created from "container images", which are much like a VM
image.
Container images are generated from a specification called a Dockerfile, which
does the job of describing the environment.

## What is `packrat`?

Packrat is a package manager for R, it does a similar job to `conda` (Python/R),
`npm` (JavaScript), `maven` (Java), `cargo` (Rust).

Importantly; it records the specific versions of packages used in your project,
so that the environment can be re-created _exactly_ on other machines.

## What is `git`?

Git is pretty much the de-facto standard code-management tool in use today.

# How does it all come together?

1. DS do their work inside the container.
2. When code is ready for deployment, `git commit && git push`.
3. CI system builds the container image and pushes it to private registry.
4. Production systems invoke that container with some command.

# Problems we've mitigated:

## Developer experience;

Ideally, devs can work in their regular environment, for R that's RStudio.

We use a container image that already has RStudio Server set up inside; so you
just fire up the container using the provided script, navigate to `localhost:8080`
in your browser, and from there you can just work as if you're using native RStudio.

Likewise, I don't need to train devs on how to use any custom-built package manager,
I can just say "go learn packrat" :D

## Container build times

Somewhat-standard for docker containers is to re-run the package installation
when building your image.
For example, during development, you install `foo==2.3.4`, which gets recorded by
your package manager, and then during the build your package manager reads its
lockfile and installs `foo==2.3.4` into the container.

This would work fine for us, if it weren't for one part:

(image: `install.packages("tidyverse")`)

Many of you will know; this takes on the order of 20 to 30 minutes, depending on
your machine and your internet connection.

The solution we came up with is actually to break the container image into 2 stages;

The first stage is called the "development" container, which _doesn't_ contain
any code or R packages.
Instead, these are made available at run-time by volume mounts.

Any R packages that are installed by the developer go into the mounted directory.

The next stage is called the "standalone" container, in which the _code and dependencies
are copied_ inside.

The result here is that:

- for development; you can use things as normal
- re-building the development container is quick, because no R dependencies are installed
- re-building the standalone container is quick, because the dependencies only need to be copied inside

of course, you would still need to install the package at-least-once, but at least
you don't need to pay that time-penalty _every time the container is built_.

# Tricky things we ran in to:

- CI: gotta make sure the right directories are cached in the VM, otherwise you
  basically need to do a completely fresh rebuild everytime (difference: 5mins
  vs 30 mins)
- Dockefile order-of-operations: can be tricky to get Docker to cache things for
  you properly
- symlinks issue: weird stuff can happen when data crosses from macOS Docker VM,
  to macOS filesystem, and back again.

# Problems we still have:

- Docker VM is actually kinda slow on macOS.
- when things _do_ break, DS have no idea how to fix

---

still to mention: container images tagged with git commit, means you can check out
the _exact_ image that's being used in production
>>>>>>> b5100a806db27075fdf3d8746a9fe599e4130a5d

I'm here today give a high-level overview of how we run R code in
production like absolute madlads.

But first, we gotta take a minute to talk about "production"
what does that mean exactly?
what's the difference between "production" versus just running it
on your machine?

deployment to production represents developers letting go of their code

% bad way to phrase; trying to say like "it needs to be able to run
% without intervention"

the big part about runnign 

one of the biggest challenges about running code without close supervision

...but we already _have_ an environment where the code works;
your machine!

so one way to ease the transition is to replicate the environment as
closely as you can

this means you need:

- the same OS
- the same system packages installed
- the same R version
- the same R package versions

and, of course, the _code_.

our solutions for these are

- docker
- docker
- docker
- packrat
- git
